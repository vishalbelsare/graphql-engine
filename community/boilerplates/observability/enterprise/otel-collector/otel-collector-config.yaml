receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

exporters:
  prometheus:
    endpoint: "0.0.0.0:8889"

  # Data sources: traces, metrics, logs
  debug:
    verbosity: detailed

  loki:
    endpoint: http://loki:3100/loki/api/v1/push

  # datadog:
  #   api:
  #     key: ${DD_API_KEY}

  ## New Relic exporter
  # otlp/newrelic:
  #   endpoint: otlp.nr-data.net:4317
  #   headers:
  #     api-key: ${NEW_RELIC_API_KEY}

  ## Export traces to Jaeger via OTLP protocol.
  ## You can also export traces directly to jaeger to graphql-engine.
  ## However, the collector supports
  otlp/jaeger:
    endpoint: jaeger:4317
    tls:
      insecure: true

  ## Honeycomb exporter
  ## Note: logs are sent to the service.name in trace (hasura), not to x-honeycomb-dataset
  # otlp/honeycomb:
  #   endpoint: api.honeycomb.io:443
  #   headers:
  #     x-honeycomb-team: ${HONEYCOMB_API_KEY}
  #     x-honeycomb-dataset: ${HONEYCOMB_DATASET}

  ## Google Cloud
  ## You need to create a service account with following roles
  ## - roles/monitoring.metricWriter
  ## - roles/cloudtrace.agent
  ## - roles/logging.logWriter
  ## download and rename to gcp-sa.json so the otel-collector can mount the service account
  ##
  # googlecloud:
  #   trace:
  #   metric:
  #     compression: gzip
  #   log:
  #     default_log_name: example-otel.io/hasura-exported-log
  #     compression: gzip

processors:
  batch:
    send_batch_max_size: 100
    send_batch_size: 10
    timeout: 10s

  transform:
    error_mode: ignore
    metric_statements:
      # OpenTelemetry doesn't automatically convert resource.attributes to label,
      # so we need to assign them to data_point attributes
      # {
      #   "resource_metrics": [
      #     {
      #       "resource": {
      #         "attributes": [
      #           {
      #             "key": "host.name",
      #             "value": {
      #               "string_value": "abc:8080"
      #             }
      #           },
      #         ]
      #       }
      #     }
      #   ]
      # }
      - context: datapoint
        statements:
          - set(attributes["instance"], resource.attributes["host.name"])

  memory_limiter:
    check_interval: 1s
    limit_percentage: 65
    spike_limit_percentage: 20

  ## New Relic can parse body in JSON string only.
  ## You must enable this transformer to convert the body to string
  ##
  # transform/new_relic_logs:
  #   log_statements:
  #     - context: log
  #       statements:
  #         - set(body, Concat([body, ""], ""))

  filter:
    error_mode: ignore
    traces:
      span:
        # You can filter unused traces to save cost, for example:
        # - 'IsMatch(name, "Event trigger")'
        # - 'IsMatch(name, "Scheduled trigger")'
        # - 'IsMatch(name, "websocket")'
        - 'IsMatch(name, "/v1/version")'
        - 'IsMatch(name, "/v1/entitlement")'
        - 'IsMatch(name, "/v1alpha1/config")'

    logs:
      log_record:
        ## Disable debug logs. You can also set HASURA_GRAPHQL_LOG_LEVEL=info.
        - severity_number == SEVERITY_NUMBER_DEBUG

        ## Deprecated log types. You should export OTEL traces and metrics instead.
        - 'attributes["type"] == "metrics" or attributes["type"] == "tracing-log" or attributes["type"] == "livequery-poller-log"'

        ## http-log and websocket-log already include most of information in query-log.
        ## You can ignore this log type unless you want to debug generated SQL.
        # - 'attributes["type"] == "query-log"'

        ## You can filter unused logs to save cost, or configure HASURA_GRAPHQL_ENABLED_LOG_TYPES in GraphQL Engine to disable unused log types
        # - 'attributes["type"] == "query-log" and IsMatch(body["query"]["operationName"], "UnknownQuery")'
        # - 'attributes["type"] == "http-log" and IsMatch(body["operation"]["query"]["operationName"], "UnknownQuery")'

  transform/logs:
    error_mode: ignore
    log_statements:
      ## Remove the transform context in trigger logs
      # - context: log
      #   conditions:
      #     - 'attributes["type"] == "scheduled-trigger" or attributes["type"] == "event-trigger"'
      #   statements:
      #     - 'delete_key(body["request"], "req_transform_ctx")'

      ## Truncate very long query strings in http-log
      - context: log
        conditions:
          - 'attributes["type"] == "http-log" and IsMap(body["operation"])'
        statements:
          - 'truncate_all(body["operation"], 4096)'

      ## Truncate very long query strings in http-log
      - context: log
        conditions:
          - 'attributes["type"] == "websocket-log" and IsMap(body["event.detail.query"])'
        statements:
          - 'truncate_all(body["event.detail.query"], 4096)'

      ## Truncate very long generated_sql strings in query-log
      - context: log
        conditions:
          - 'attributes["type"] == "query-log" and IsMap(body["generated_sql"])'
        statements:
          - 'truncate_all(body["generated_sql"], 4096)'

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [filter, memory_limiter, batch]
      exporters: [otlp/jaeger, debug]

    logs:
      receivers: [otlp]
      processors: [filter, transform/logs, memory_limiter, batch]
      exporters: [loki, debug]

    metrics:
      receivers: [otlp]
      processors: [memory_limiter, transform, batch]
      exporters: [prometheus]
